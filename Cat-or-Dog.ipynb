{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b61a67-a872-4c23-9591-66c9d83dfc40",
   "metadata": {},
   "source": [
    "# Project - Cat or Dag Classifier\n",
    "# Table of Content\n",
    "<ul>\n",
    "    <li><a href='#Introduction'>Introduction</a></li>\n",
    "    <li><a href='#Dataset'>Dataset</a></li>\n",
    "    <li><a href='#Dataset'>Data Preprocessing</a></li>\n",
    "    <li><a href='#Dataset'>Model Building</a></li>\n",
    "    <li><a href='#Dataset'>Model Training</a></li>\n",
    "    <li><a href='#Dataset'>Model Evaluation</a></li>\n",
    "    <li><a href='#Dataset'>Conclusion</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d98d73a-d72e-4f55-83fc-5d71ebd11ba3",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<a id='Introduction'></a>\n",
    "This project focuses on creating a deep learning model that can classify images as either cat or dog. Using a convolutional neural network (CNN), the model is trained to recognize and differentiate between these two animals by learning from a dataset of labeled images.\n",
    "\n",
    "# Dataset\n",
    "<a id='Dataset'></a>\n",
    "This dataset consists of two image classes: cats and dogs, making it ideal for binary classification tasks in computer vision. The dataset can be effectively used to train models that distinguish between these two animal classes, providing a strong foundation for exploring deep learning applications in image recognition.<br>\n",
    "<br>\n",
    "The dataset contains a total of 14,000 images, with an equal distribution between cats and dogs. For training, there are 5,000 images of dogs and 5,000 images of cats, allowing the model to learn from a balanced set of examples. The remaining 4,000 images are reserved for testing, comprising 2,000 images of dogs and 2,000 images of cats. <br>\n",
    "<br>\n",
    "The link of the dataset \n",
    "<a href=\"https://www.kaggle.com/datasets/anthonytherrien/dog-vs-cat\">Dog vs Cat</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64184265-930a-469a-bbcc-03b260af3d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e960c11b-64e6-4ed8-95ff-fc7271e28bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c905220-45f4-4f18-9e19-4ac9caad1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.ImageFolder(root='data/train', transform=transform)\n",
    "test_dataset = datasets.ImageFolder(root=\"data/test\", transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b26d4fe-690f-483f-ab69-ba2d55900665",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d36ccd26-28b7-49a8-9071-197d89f695da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5) \n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "\n",
    "        x = x.view(-1, 256 * 14 * 14)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "613b2064-dc2c-4548-bef5-599a168a4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a14d99a-4f46-43c5-8c75-a31154c50038",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[0;32m     12\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_grid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36mimshow\u001b[1;34m(img, title)\u001b[0m\n\u001b[0;32m      3\u001b[0m npimg \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mtranspose(npimg, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m title \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mtitle(title)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img, title=None):\n",
    "    img = img * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    npimg = img.numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae381dee-747c-42d5-a0c2-205737f95fed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
